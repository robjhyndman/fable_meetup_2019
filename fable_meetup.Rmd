---
title: "Tidy forecasting in&nbsp;R"
date: "26 September 2019"
author: "Rob J Hyndman"
toc: true
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(digits = 3, width = 60)
library(fpp3)
global_economy <- global_economy %>%
  select(Year, Country, GDP, Imports, Exports, Population)
tourism <- tourism %>%
  mutate(
    State = recode(State,
                   "Australian Capital Territory" = "ACT",
                   "New South Wales"="NSW",
                   "Northern Territory" = "NT",
                   "Queensland" = "QLD",
                   "South Australia" = "SA",
                   "Tasmania" = "TAS",
                   "Victoria"="VIC",
                   "Western Australia" = "WA"
    )
  )
```

# Tidy time series data

## Tidyverts packages

\begin{textblock}{3.8}(8,0)\begin{alertblock}{}\Large\textbf{tidyverts.org}\end{alertblock}\end{textblock}

\placefig{1}{1.4}{width=4cm}{tsibble.png}
\placefig{5}{1.4}{width=4cm}{tsibbledata.png}
\placefig{3}{4.85}{width=4cm}{feasts.png}
\placefig{7}{4.85}{width=4cm}{fable.png}

## Time series data

  - Four-yearly Olympic winning times
  - Annual Google profits
  - Quarterly Australian snowy production
  - Monthly rainfall
  - Weekly retail sales
  - Daily IBM stock prices
  - Hourly electricity demand
  - 5-minute freeway traffic counts
  - Time-stamped stock transaction data

## `tsibble` objects

\fontsize{10}{11.2}\sf

```{r, echo = TRUE}
global_economy
```

\only<2->{\begin{textblock}{.75}(2.15,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Index\phantom{dg}\end{alertblock}
\end{textblock}}
\only<3->{\begin{textblock}{1.6}(3.28,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Key\phantom{dg}\end{alertblock}
\end{textblock}}
\only<4>{\begin{textblock}{6.7}(5.5,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Measured variables\phantom{dg}\end{alertblock}
\end{textblock}}

## `tsibble` objects

\fontsize{10}{11.3}\sf

```{r, echo = TRUE}
tourism
```

\only<2->{\begin{textblock}{1.1}(2.1,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Index\phantom{dg}\end{alertblock}
\end{textblock}}
\only<3->{\begin{textblock}{3.9}(3.65,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Keys\phantom{dg}\end{alertblock}
\end{textblock}}
\only<4-5>{\begin{textblock}{1.5}(7.95,3.7)
\begin{alertblock}{}\fontsize{10}{10}\sf Measure\phantom{dg}\end{alertblock}
\end{textblock}}

\only<5>{\begin{textblock}{3}(9,5)
\begin{block}{}\fontsize{10}{10}\sf Domestic visitor nights in thousands by state/region and purpose.\phantom{dg}\end{block}
\end{textblock}}

## `tsibble` objects

* A `tsibble` allows storage and manipulation of multiple time series in R.

* It contains:

  + An index: time information about the observation
  + Measured variable(s): numbers of interest
  + Key variable(s): optional unique identifiers for each series

* It works with tidyverse functions.

## Extracting single time series
\fontsize{11}{12}\sf

```{r extract}
snowy <- tourism %>%
  filter(
    Region=="Snowy Mountains",
    Purpose=="Holiday"
  )
snowy
```

## Extracting single time series
\fontsize{11}{12}\sf

```{r extract2}
snowy %>% autoplot(Trips)
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these series?}
\end{alertblock}
\end{textblock}

# Benchmark forecasting methods

## Benchmark forecasting methods
\fontsize{13}{14}\sf

### Mean method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

```{r mean-method-explained, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 3.3, dependson='snowy'}
fc <- snowy %>%
  filter(!is.na(Trips)) %>%
  model(MEAN(Trips)) %>%
  forecast(h="5 years")

snowy %>%
  mutate(average = mean(Trips)) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  geom_line(aes(y = average), colour = "blue", linetype = "dashed") +
  geom_line(aes(x=Quarter, y=Trips), color='blue', data=fc) +
  ggtitle("Quarterly holidays to Snowy Mountains")
```

## Benchmark forecasting methods
\fontsize{13}{14}\sf

### Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.\phantom{$\hat{y}_{T+h|T}$}

```{r naive-method-explained, echo = FALSE, warning = FALSE, fig.height = 3.3, dependson='snowy'}
snowy %>%
  filter(!is.na(Trips)) %>%
  model(NAIVE(Trips)) %>%
  forecast(h = "5 years") %>%
  autoplot(snowy, level = NULL) +
  geom_point(data = slice(snowy, n()), colour = "blue") +
      ggtitle("Quarterly holidays to Snowy Mountains")
```

## Benchmark forecasting methods
\fontsize{13}{14}\sf

### Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-m(k+1)}$, where $m=$ seasonal period and $k$ is the integer part of $(h-1)/m$.\phantom{$\hat{y}_{T+h|T}$}

```{r snaive-method-explained, echo = FALSE, warning = FALSE, fig.height = 3.3, dependson='snowy'}
snowy %>%
  model(SNAIVE(Trips ~ lag("year"))) %>%
  forecast(h = "5 years") %>%
  autoplot(snowy, level = NULL) +
  geom_point(data = slice(snowy, (n()-3):n()), colour = "blue") +
      ggtitle("Quarterly holidays to Snowy Mountains")
```

## Benchmark forecasting methods
\fontsize{13}{14}\sf

### Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts: $\hat{y}_{T+h|T}  = y_T + \frac{h}{T-1}(y_T -y_1)$.
 * Equivalent to line between first and last observations.\rlap{\phantom{$\hat{y}_{T+h|T}$}}

```{r drift-method-explained, echo = FALSE, warning = FALSE, fig.height=3.3, dependson='snowy'}
snowy %>%
  model(RW(Trips ~ drift())) %>%
  forecast(h = "5 years") %>%
  autoplot(snowy, level = NULL) +
  geom_line(data = slice(snowy, range(cumsum(!is.na(Trips)))),
            linetype = "dashed", colour = "blue") +
  ggtitle("Quarterly holidays to Snowy Mountains")
```

## Model estimation

The `model()` function trains models to data.

\fontsize{9}{9}\sf

```{r snowy-model}
# Fit the models
fit <- snowy %>%
  model(
    Mean = MEAN(Trips),
    `Naïve` = NAIVE(Trips),
    `SeasonalNaïve` = SNAIVE(Trips),
    Drift = RW(Trips ~ drift())
  )
```

```{r snowy-mable, echo = TRUE, dependson='snowy-model'}
fit
```

###
A `mable` is a model table, each cell corresponds to a fitted model.

## Producing forecasts

\fontsize{10}{13}\sf

```{r snowy-fc, echo = TRUE, dependson='snowy-model'}
fc <- fit %>%
  forecast(h = 12)
```

```{r snowy-fbl, echo = FALSE, dependson='Trips-fc'}
print(fc, n = 4)
```

###
A `fable` is a forecast table with point forecasts and distributions.

## Visualising forecasts

\footnotesize

```{r snowy-fc-plot, warning=FALSE, message=FALSE, fig.height=3, dependson='snowy-fc'}
fc %>%
  autoplot(snowy, level = NULL) +
  ggtitle("Forecasts for Snowy Mountains holidays") +
  xlab("Year") +
  guides(colour=guide_legend(title="Forecast"))
```

## Forecasting many series
\fontsize{10}{12}\sf

```{r scaleup1}
tourism
```

## Forecasting many series
\fontsize{10}{12}\sf

```{r scaleup2}
tourism %>%
  model(
    mean = MEAN(Trips),
    snaive = SNAIVE(Trips)
  )
```

## Forecasting many series
\fontsize{10}{12}\sf

```{r scaleup3}
tourism %>%
  model(
    mean = MEAN(Trips),
    snaive = SNAIVE(Trips)
  ) %>%
  forecast(h= "3 years")
```

# Exponential smoothing

## Historical perspective

 * Developed in the 1950s and 1960s as methods (algorithms) to produce point forecasts.
 * Combine "level", "trend" (slope) and "seasonal" states to describe a time series.
 * The rate of change of the components are controlled by "smoothing parameters".
  * Need to choose best values for the smoothing parameters and initial states.
  * Equivalent ETS state space models developed in the 1990s and 2000s.

## ETS state space models

\begin{block}{}
\hspace*{-0.25cm}\begin{tabular}{l@{}p{2.3cm}@{}c@{}l}
\structure{General n\rlap{otation}}
    &       & ~E T S~  & ~:\hspace*{0.3cm}\textbf{E}xponen\textbf{T}ial \textbf{S}moothing               \\ [-0.2cm]
    & \hfill{$\nearrow$\hspace*{-0.1cm}}        & {$\uparrow$} & {\hspace*{-0.2cm}$\nwarrow$} \\
    & \hfill{\textbf{E}rror\hspace*{0.2cm}} & {\textbf{T}rend}      & {\hspace*{0.2cm}\textbf{S}eason}
\end{tabular}
\end{block}

\alert{\textbf{E}rror:} Additive (`"A"`) or multiplicative (`"M"`)
\pause

\alert{\textbf{T}rend:} None (`"N"`), additive (`"A"`), multiplicative (`"M"`), or damped (`"Ad"` or `"Md"`).
\pause

\alert{\textbf{S}easonality:} None (`"N"`), additive (`"A"`) or multiplicative (`"M"`)

## ETS state space models
\fontsize{11}{12}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Additive Error}} &        \multicolumn{3}{c}{\bf Seasonal Component}         \\
          \multicolumn{2}{c|}{\bf Trend}         &         N         &         A         &         M         \\
        \multicolumn{2}{c|}{\bf Component}       &     ~(None)~      &    (Additive)     & (Multiplicative)  \\ \cline{3-5}
           &                                     &                   &                   &  \\[-0.3cm]
  N        & (None)                              &       A,N,N       &       A,N,A       &    \st{A,N,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A        & (Additive)                          &       A,A,N       &       A,A,A       &    \st{A,A,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                   &   A,A\damped,N    &   A,A\damped,A    & \st{A,A\damped,M}
\end{tabular}
\end{block}

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Multiplicative Error}} &     \multicolumn{3}{c}{\bf Seasonal Component}      \\
             \multicolumn{2}{c|}{\bf Trend}            &      N       &         A         &        M         \\
           \multicolumn{2}{c|}{\bf Component}          &   ~(None)~   &    (Additive)     & (Multiplicative) \\ \cline{3-5}
           &                                           &              &                   &  \\[-0.3cm]
  N        & (None)                                    &    M,N,N     &       M,N,A       &      M,N,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A        & (Additive)                                &    M,A,N     &       M,A,A       &      M,A,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                         & M,A\damped,N &   M,A\damped,A    &   M,A\damped,M
\end{tabular}
\end{block}

## ETS state space models
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=2.2cm]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=25pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    \node[input neuron] (x0) at (0,-1) {$\bm{x}_{t-1}$};
    \node[input neuron] (eps0) at (0,-2) {$\varepsilon_{t}$};
    \node[output neuron, right of=x0] (y1) {$y_t$};
	\path (x0) edge (y1);
	\path (eps0) edge (y1);
\only<2->{\node[input neuron, right of=eps0] (x1) {$\bm{x}_{t}$};
	\path (x0) edge (x1);
	\path (eps0) edge (x1);}
\only<3->{\node[output neuron, right of=x1] (y2) {$y_{t+1}$};
    \node[input neuron, below of=x1, node distance=1cm] (eps1) {$\varepsilon_{t+1}$};
	\path (x1) edge (y2);
	\path (eps1) edge (y2);}
\only<4->{\node[input neuron, right of=eps1] (x2) {$\bm{x}_{t+1}$};
	\path (x1) edge (x2);
	\path (eps1) edge (x2);}
\only<5->{\node[output neuron, right of=x2] (y3) {$y_{t+2}$};
    \node[input neuron, below of=x2, node distance=1cm] (eps2) {$\varepsilon_{t+2}$};
	\path (x2) edge (y3);
	\path (eps2) edge (y3);}
	
\only<6->{\node[input neuron, right of=eps2] (x3) {$\bm{x}_{t+2}$};
	\path (x2) edge (x3);
	\path (eps2) edge (x3);}
\only<7->{\node[output neuron, right of=x3] (y4) {$y_{t+3}$};
    \node[input neuron, below of=x3, node distance=1cm] (eps3) {$\varepsilon_{t+3}$};
	\path (x3) edge (y4);
	\path (eps3) edge (y4);}

\only<8->{\node[input neuron, right of=eps3] (x4) {$\bm{x}_{t+3}$};
	\path (x3) edge (x4);
	\path (eps3) edge (x4);}
\only<9->{\node[output neuron, right of=x4] (y5) {$y_{t+4}$};
    \node[input neuron, below of=x4, node distance=1cm] (eps4) {$\varepsilon_{t+4}$};
	\path (x4) edge (y5);
	\path (eps4) edge (y5);}
	
\end{tikzpicture}

\vspace*{10cm}

{\begin{textblock}{5.99}(6.5,1.2)
\begin{block}{State space model}\small
$\bm{x}_t = (\text{level}, \text{slope}, \text{seasonal})$
\end{block}
\end{textblock}}

\only<10->{\begin{textblock}{6}(0.5,5.5)
\begin{alertblock}{Estimation}\small
Compute likelihood $L$ from $\varepsilon_1,\varepsilon_2,\dots,\varepsilon_T$.

Optimize $L$ wrt model parameters.
\end{alertblock}
\end{textblock}}



## Automatic forecasting

**From Hyndman et al.\ (IJF, 2002):**

1. Apply each model that is appropriate to the data.
Optimize parameters and initial values using MLE.
1. Select best method using AICc.
1. Produce forecasts using best method.
1. Obtain forecast intervals using underlying state space model.

* Method performed very well in M3 competition.
* Used as a benchmark in the M4 competition.

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-fit, echo=TRUE}
fit <- tourism %>% model(ets = ETS(Trips))
fit
```

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-report}
fit %>% filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  report()
```

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-components}
fit %>% filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  components(fit)
```

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-components-plot, fig.height=4.3}
fit %>% filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  components(fit) %>% autoplot()
```

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-forecast}
fit %>% forecast()
```

## Example: Australian tourism

\fontsize{9}{10}\sf

```{r ausholidays-forecast-plot, dependson='ausholidays-fit'}
fit %>% forecast() %>%
  filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  autoplot(tourism) +
    xlab("Year") + ylab("Overnight trips (thousands)")
```

# ARIMA models

## ARIMA models

\begin{tabular}{rl}
\textbf{AR}: & autoregressive (lagged observations as inputs)\\
\textbf{I}: & integrated (differencing to make series stationary)\\
\textbf{MA}: & moving average (lagged errors as inputs)
\end{tabular}

\pause

###
An ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.


## ARIMA models

\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=25pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{annot} = [text width=4em, text centered]

	% AR terms
    \node[output neuron] (ar1) at (0,-1) {$y_{t-1}$};
    \node[output neuron] (ar2) at (0,-2) {$y_{t-2}$};
    \node[output neuron] (ar3) at (0,-3) {$y_{t-3}$};
    \only<2->{\node[input neuron] (ma0) at (0,-4) {$\varepsilon_{t}$};}
    \only<3->{\node[input neuron] (ma1) at (0,-5) {$\varepsilon_{t-1}$};
    \node[input neuron] (ma2) at (0,-6) {$\varepsilon_{t-2}$};}
%    \node[input neuron] (time) at (0,-7) {$t$};

    % Draw the output layer node
    \node[output neuron, right of=ar2, yshift=-0.5cm] (y) {$y_t$};

    % Connect every node in the  layer with the output layer
	\path (ar1) edge (y);
	\path (ar2) edge (y);
	\path (ar3) edge (y);
	\only<3->{\path (ma1) edge (y);
	\path (ma2) edge (y);}
	\only<2->{\path (ma0) edge (y);}
%	\path (time) edge (y);
	
    % Annotate the layers
    \node[annot] (input) {Inputs};
    \node[annot,right of=input] {Output};
\end{tikzpicture}

\vspace*{10cm}

\only<2->{\begin{textblock}{6}(6,1)
\begin{block}{}
\only<2>{Autoregression (AR) model}
\only<3->{Autoregression moving average (ARMA) model}
\end{block}
\end{textblock}}

\only<4->{\begin{textblock}{6}(6,5.6)
\begin{alertblock}{Estimation}
Compute likelihood $L$ from $\varepsilon_1,\varepsilon_2,\dots,\varepsilon_T$.

Use optimization algorithm to maximize $L$.
\end{alertblock}
\end{textblock}}


\only<5->{\begin{textblock}{6}(6,2.7)
\begin{block}{ARIMA model}
Autoregression moving average (ARMA) model applied to 
differences.
\end{block}
\end{textblock}}

## Seasonal ARIMA models
\fontsize{12}{13}\sf

\begin{block}{}\centering
\begin{tabular}[]{@{}rcc@{}}
ARIMA & \(~\underbrace{(p, d, q)}\) &
\(\underbrace{(P, D, Q)_{m}}\)\tabularnewline
\midrule
& \({\uparrow}\) & \({\uparrow}\)\tabularnewline
& Non-seasonal part & Seasonal part of\tabularnewline
& of the model & of the model\tabularnewline
\end{tabular}
\end{block}

\vspace*{-0.4cm}

  * $m =$ number of observations per year.
  * $d$ first differences, $D$ seasonal differences
  * $p$ AR lags, $q$ MA lags
  * $P$ seasonal AR lags, $Q$ seasonal MA lags
  
\pause

\begin{alertblock}{Hyndman and Khandakar (JSS, 2008) algorithm:}
\begin{itemize}\tightlist
\item Select no.\ differences $d$ and $D$ via tests.
\item Select model orders $p$, $q$, $P$, $Q$ by minimising AICc.
\item Use stepwise search to traverse model space.
\end{itemize}
\end{alertblock}

## Example: Australian tourism
\fontsize{10}{11}\sf

```{r popfit2, echo=TRUE, cache=TRUE}
fit <- tourism %>%
  model(arima = ARIMA(Trips))
fit
```

## Example: Australian tourism
\fontsize{9}{9}\sf

```{r popfc2, echo=TRUE, cache=TRUE}
fit %>% forecast(h=10) %>%
  filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  autoplot(tourism)
```

# Forecast accuracy measures

## Training and test sets

```{r traintest, fig.height=1, echo=FALSE, cache=TRUE}
train = 1:18
test = 19:24
par(mar=c(0,0,0,0))
plot(0,0,xlim=c(0,26),ylim=c(0,2),xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
arrows(0,0.5,25,0.5,0.05)
points(train, train*0+0.5, pch=19, col="blue")
points(test,  test*0+0.5,  pch=19, col="red")
text(26,0.5,"time")
text(10,1,"Training data",col="blue")
text(21,1,"Test data",col="red")
```

  * A model which fits the training data well will not necessarily forecast well.
  * Forecast accuracy is based only on the test set.

### Forecast errors

Forecast "error": the difference between an observed value and its forecast.
$$
  e_{T+h} = y_{T+h} - \hat{y}_{T+h|T},
$$
where the training data is given by $\{y_1,\dots,y_T\}$

## Forecast errors

```{r trainall, echo=TRUE}
train <- tourism %>%
  filter(year(Quarter) <= 2014)
fit <- train %>%
  model(
    ets = ETS(Trips),
    arima = ARIMA(Trips),
    snaive = SNAIVE(Trips)
  ) %>%
  mutate(mixed = (ets+arima+snaive)/3) 
fc <- fit %>% forecast(h="3 years") 
```

## Forecast errors
\fontsize{11}{12}\sf

```{r snowytrain, dependson='trainall'}
fc %>% 
  filter(Region=="Snowy Mountains", Purpose=="Holiday") %>%
  autoplot(level=NULL) +
  autolayer(snowy, Trips)
```

## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{align*}
\text{MAE} &= \text{mean}(|e_{T+h}|) \\[-0.2cm]
\text{MSE} &= \text{mean}(e_{T+h}^2) \qquad
&&\text{RMSE} &= \sqrt{\text{mean}(e_{T+h}^2)} \\[-0.1cm]
\text{MAPE} &= 100\text{mean}(|e_{T+h}|/ |y_{T+h}|)
\end{align*}\pause

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For non-seasonal time series,
$$
  Q = (T-1)^{-1}\sum_{t=2}^T |y_t-y_{t-1}|
$$
works well. Then MASE is equivalent to MAE relative to a naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For seasonal time series,
$$
  Q = (T-m)^{-1}\sum_{t=m+1}^T |y_t-y_{t-m}|
$$
works well. Then MASE is equivalent to MAE relative to a seasonal naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\fontsize{10}{10}\sf

```{r snowy-test-accuracy, dependson='trainall'}
accuracy(fc, tourism)
```


## Measures of forecast accuracy

\fontsize{10}{10}\sf

```{r snowy-test-accuracy2, dependson='trainall'}
accuracy(fc, tourism) %>%
  group_by(.model) %>%
  summarise(
    RMSE = mean(RMSE),
    MAE = mean(MAE),
    MASE = mean(MASE)
  ) %>%
  arrange(RMSE)
```

## Acknowledgements

\begin{block}{}
\fontsize{11}{11}\sf
\centering\begin{tabular}{l@{\hspace*{1cm}}l}
\includegraphics[height=4cm, width=10cm]{mitch2} &
\includegraphics[height=4cm, width=10cm]{earowang} \\
Mitchell O'Hara-Wild &
Earo Wang \\
\end{tabular}
\end{block}\vspace*{-0.15cm}

\begin{alertblock}{}
\centerline{\textbf{tidyverts.org}}
\centerline{\textbf{robjhyndman.com}}
\end{alertblock}


\vspace*{10cm}